{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
    "\n",
    "# import the SentenceTransformer, a wrapper on top of the model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# get 'sentence-transformers/multi-qa-mpnet-base-cos-v1', a pretrained model\n",
    "model = SentenceTransformer(\n",
    "  'sentence-transformers/multi-qa-mpnet-base-cos-v1'\n",
    ")\n",
    "\n",
    "docs = [\n",
    "  \"A paragon of virtue\",\n",
    "  \"The hero of legend\"\n",
    "]\n",
    "\n",
    "# Note that there is a limit of 512 word pieces:\n",
    "# Text longer than that will be truncated.\n",
    "# Further note that the model was just trained on\n",
    "# input text up to 250 word pieces.\n",
    "# It might not work well for longer text.\n",
    "\n",
    "embeddings = model.encode(\n",
    "  docs\n",
    "  , batch_size=32\n",
    "  , show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 84/84 [00:07<00:00, 10.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def makeParagraphs(text):\n",
    "  '''\n",
    "  creates a list of paragraphs.\n",
    "  text: a string of arbitrary length.\n",
    "  '''\n",
    "  paragraph_separator_re = re.compile(r'(\\.\\n\\r?)+(\\n\\r?)*')\n",
    "  return re.split(paragraph_separator_re, text)\n",
    "\n",
    "def removeEmpty(chunks):\n",
    "  '''\n",
    "  removes chunks that only contain dots or new lines\n",
    "  '''\n",
    "  return [chunk.strip() for chunk in chunks if chunk and len(chunk.strip('.\\n\\t ')) > 0]\n",
    "\n",
    "def split(chunk, size_limit=150):\n",
    "  '''\n",
    "  splits chunks so that they contain no more than 150 words or the chosen limit\n",
    "  '''\n",
    "  if (len(chunk.split(' ')) > size_limit):\n",
    "    periods = re.compile(r'\\.|\\?|;')\n",
    "    return re.split(periods, chunk)\n",
    "  return [chunk]\n",
    "\n",
    "def rightSize(chunks, size_limit=150):\n",
    "  '''\n",
    "  creates new chunks if needed so that chunks do not exceed the size limit.\n",
    "  '''\n",
    "  batch = []\n",
    "  for chunk in chunks:\n",
    "    rightsized_chunks = split(chunk, size_limit)\n",
    "    for right_sized_chunk in rightsized_chunks:\n",
    "      batch.append(right_sized_chunk)\n",
    "  return batch\n",
    "\n",
    "def makeEmbedding(chunks, model=model):\n",
    "  '''\n",
    "  makes embeddings out of a group of chunks\n",
    "  model is 'sentence-transformers/multi-qa-mpnet-base-cos-v1' and warpped in SentenceTransformer\n",
    "  '''\n",
    "  return model.encode(\n",
    "    chunks\n",
    "    , batch_size=32\n",
    "    , device='mps' # send work to Metal shaders in M1 macs\n",
    "    , show_progress_bar=True\n",
    "  )\n",
    "\n",
    "def makeChunks(raw_text):\n",
    "  '''\n",
    "  creates chunks out of raw text. Chunks will have default length\n",
    "  '''\n",
    "  paragraphs = removeEmpty(makeParagraphs(raw_text))\n",
    "  return rightSize(paragraphs)\n",
    "\n",
    "def fileToChunks(filePath, encoding='utf-8'):\n",
    "  '''\n",
    "  turns a file of raw text into chunks that are rightsized\n",
    "  '''\n",
    "  with open(filePath, encoding=encoding) as file:\n",
    "    raw_text = file.read()\n",
    "    return makeChunks(raw_text)\n",
    "\n",
    "def makeEmbeddingsPerChunk(forFile):\n",
    "  '''\n",
    "  makes embeddings out of a file and returns chunks per file and its respective embeddings\n",
    "  '''\n",
    "  chunks = fileToChunks(forFile)\n",
    "  embeddings = makeEmbedding(chunks)\n",
    "  return { chunks[i]:embeddings[i] for i in range(len(chunks))}\n",
    "\n",
    "# these are ready to get inserted into a database:\n",
    "embeddings = makeEmbeddingsPerChunk('pg2680.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2615 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2615/2615 [00:06<00:00, 424.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# insert embeddings into the database\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATABASE = \"semantic_search\"\n",
    "HOST = \"127.0.0.1\"\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"123456\"\n",
    "CMD = \"\"\"\n",
    "insert into \n",
    "  items(embedding, text_chunk)\n",
    "  values (%s, %s)\n",
    "  returning id;\n",
    "\"\"\"\n",
    "\n",
    "with psycopg2.connect(\n",
    "    host=HOST,\n",
    "    database=DATABASE,\n",
    "    user=USER,\n",
    "    password=PASSWORD\n",
    "  ) as connection:\n",
    "    with connection.cursor() as cursor:\n",
    "      for txt in tqdm(embeddings.keys()):\n",
    "        if len(txt) > 1024:\n",
    "           print('too big: %s' % txt)\n",
    "           continue\n",
    "        cursor.execute(CMD, (embeddings[txt].tolist(), txt))\n",
    "        id = cursor.fetchone()[0]  \n",
    "      cursor.close()\n",
    "      connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' which of all the virtues is the proper\\nvirtue for this present use',), (' This dictum might easily be taken to mean that virtue consists\\nin yielding to each natural impulse',), (' All these things are merely the sphere in which\\nvirtue may act',), (' This conforming of the life to nature was the Stoic idea of\\nVirtue',), (' For each fault in others, Nature (says\\nhe) has given us a counteracting virtue',)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# query the book\n",
    "q = \"what is virtue?\"\n",
    "q_encoded = makeEmbedding([q])[0].tolist()\n",
    "CMD = \"select text_chunk from items order by embedding <=> '%s' limit 5\"\n",
    "results = None\n",
    "with psycopg2.connect(\n",
    "    host=HOST,\n",
    "    database=DATABASE,\n",
    "    user=USER,\n",
    "    password=PASSWORD\n",
    "  ) as connection:\n",
    "    with connection.cursor() as cursor:\n",
    "      cursor.execute(CMD % q_encoded)\n",
    "      results = cursor.fetchall()\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch211",
   "language": "python",
   "name": "pytorch211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
