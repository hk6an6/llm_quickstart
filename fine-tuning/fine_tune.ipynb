{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Download amazon reviews and place it into a pandas dataframe.\n",
    "\n",
    "### Download amazon reviews\n",
    "\n",
    "Get the reviews from https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi?resource=download. Reviews were previously posted at https://huggingface.co/datasets/amazon_reviews_multi but they were taken down by request of the data owner. Extract the reviews and place them in a folder named \"reviews\", next to this file.\n",
    "\n",
    "### Get reviews into a Pandas dataframe\n",
    "\n",
    "See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH=\"./reviews/%s.csv\"\n",
    "FILES=[\n",
    "  \"train\",\n",
    "  \"test\",\n",
    "  \"validation\"]\n",
    "PREPARED_OUTPUT_PATH = \"amazon-english-full-%s-sentiment.jsonl\"\n",
    "\n",
    "# load \n",
    "training_df = pd.read_csv('./reviews/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ./reviews/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/t48zyp4j3c3721sqt3jvgslr0000gn/T/ipykernel_50984/559640137.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  english_df.drop_duplicates(subset=['prompt'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ./reviews/train.csv\n",
      "loading ./reviews/test.csv\n",
      "saved ./reviews/test.csv\n",
      "loading ./reviews/validation.csv\n",
      "saved ./reviews/validation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/t48zyp4j3c3721sqt3jvgslr0000gn/T/ipykernel_50984/559640137.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  english_df = df[training_df['language'] == 'en']\n",
      "/var/folders/6m/t48zyp4j3c3721sqt3jvgslr0000gn/T/ipykernel_50984/559640137.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  english_df = df[training_df['language'] == 'en']\n"
     ]
    }
   ],
   "source": [
    "def prepare_df_for_training(df:pd.DataFrame)->pd.DataFrame:\n",
    "  '''\n",
    "  returns records in english and without duplicates. Records \\\n",
    "  follow openAIs format for tuning models.\n",
    "  '''\n",
    "  df['prompt'] = df['review_title'] + '\\n\\n' + df['review_body']\n",
    "  df['completion'] = df['stars'].astype(str)\n",
    "  english_df = df[training_df['language'] == 'en']\n",
    "  english_df.drop_duplicates(subset=['prompt'], inplace=True)\n",
    "  return english_df[['prompt','completion']].sample(len(english_df))\n",
    "\n",
    "def create_jsonl_files()->None:\n",
    "  for i in range(len(FILES)):\n",
    "    file_path = PATH % FILES[i]\n",
    "    print('loading %s' % file_path)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = prepare_df_for_training(df)\n",
    "    df.to_json(PREPARED_OUTPUT_PATH\n",
    "               % FILES[i], orient='records', lines=True)\n",
    "    print(\"saved %s\" % file_path)\n",
    "\n",
    "# prepare data frames for use with open AI\n",
    "create_jsonl_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, 3 files will be placed in your local directory. These files use a format that is compatible with OpenAIs fine tuning API.\n",
    "\n",
    "If you are using OpenAI to fine tune a model, then use `conda env config vars set OPENAI_API_KEY=<your_api_key>`.\n",
    "\n",
    "I am not going to use OpenAI and instead will rely on huggingface to fine tune mistral7b. Run the following commands:\n",
    "\n",
    "1. `pip install -U autotrain-advanced`\n",
    "1. `pip install datasets transformers`\n",
    "\n",
    "More context and examples for how mistral was trained to follow instructions are in https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this instruction will be used to predict the rating for every review\n",
    "INSTRUCTION = \"\\\n",
    "You are a customer reviewing a product that you purchased \\\n",
    "and your task is to rate how you feel about the product based \\\n",
    "on your review. The rating is a number in the range of 1 to 5 \\\n",
    "where 1 is extremely negative and 5 is extremely positive.\"\n",
    "\n",
    "# this will be used to prompt mistral7b during training\n",
    "TEXT =\"\\\n",
    "<s>[INST] Below is an instruction that describes a task, \\\n",
    "paired with an input that provides context for the task. \\\n",
    "Write a response that appropriately completes the request. \\\n",
    "\\n\\n### Instruction:\\n%s \\\n",
    "\\n\\n### Input:\\n <product_review>%s</product_review> [/INST] \\\n",
    "%s</s>\"\n",
    "\n",
    "def format_text(data:pd.Series)->str:\n",
    "  '''\n",
    "  Takes an object indexed by the columns and produces a new value.\n",
    "  data: a pandas series with indexes \"prompt\" and \"completion\"\n",
    "  '''\n",
    "  return TEXT % (INSTRUCTION,\n",
    "                 data[\"prompt\"],\n",
    "                 data[\"completion\"])\n",
    "\n",
    "def create_csv_for_mistral(fileNames:list[str]=[\"train\"])->None:\n",
    "  '''\n",
    "  persists a data frame to train mistral, given a jsonl file with\n",
    "  prompt and completion columns used to train openAIs chaptGPT.\n",
    "  fileNames: list of files to load\n",
    "  '''\n",
    "  for filename in fileNames:\n",
    "    path = PREPARED_OUTPUT_PATH % filename\n",
    "    df = pd.read_json(path,lines=True)\n",
    "    df[\"text\"] = df.apply(format_text, axis=1)\n",
    "    df[\"instruction\"] = pd.Series(\n",
    "      [INSTRUCTION for i in range(len(df.index))])\n",
    "    df[\"input\"] = df[\"prompt\"]\n",
    "    df[\"output\"] = df[\"completion\"]\n",
    "    df[[\"instruction\",\"input\",\"output\",\"text\"]]\\\n",
    "      .to_csv('data/train.csv', index=False)\n",
    "    \n",
    "create_csv_for_mistral()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to fine-tune mistral7B:instruct! (actually https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "We'll use HuggingFace's AutoTrain to fine-tune mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
      "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, add_eos_token=False, block_size=1024, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.045, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision=None, quantization=None, model_max_length=1024, trainer='default', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, apply_chat_template=False, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token=None, repo_id=None, push_to_hub=False, model='mistralai/Mistral-7B-Instruct-v0.2', project_name='my_autotrained_sentiment_predictor_llm', seed=42, epochs=4, gradient_accumulation=4, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='data/', train_split='train', valid_split=None, batch_size=4, func=<function run_llm_command_factory at 0x29470f880>)\u001b[0m\n",
      "> \u001b[1mINFO    Dataset: my_autotrained_sentiment_predictor_llm (lm_training)\n",
      "Train data: ['data//train.csv']\n",
      "Valid data: []\n",
      "Column mapping: {'text': 'text', 'rejected_text': 'rejected', 'prompt': 'prompt'}\n",
      "\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 199859/199859 [00:00<00:00, 2506659.06 \n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 199859/199859 [00:00<00:00, 2079851.04 \n",
      "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
      "> \u001b[1mINFO    {\"model\":\"mistralai/Mistral-7B-Instruct-v0.2\",\"project_name\":\"my_autotrained_sentiment_predictor_llm\",\"data_path\":\"my_autotrained_sentiment_predictor_llm/autotrain-data\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":1024,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":null,\"lr\":0.0002,\"epochs\":4,\"batch_size\":4,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"apply_chat_template\":false,\"quantization\":null,\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.045,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"autotrain_prompt\",\"text_column\":\"autotrain_text\",\"rejected_text_column\":\"autotrain_rejected_text\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
      "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'my_autotrained_sentiment_predictor_llm/training_params.json']\u001b[0m\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-01 18:58:01\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-01 18:58:01\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'autotrain_text'],\n",
      "    num_rows: 199859\n",
      "})\u001b[0m\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-01 18:58:01\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:29<00:00,  9.79s/it]\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-01 18:58:33\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "Running tokenizer on train dataset:  24%|â–| 48000/199859 [00:02<00:07, 19149.49 Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on train dataset: 100%|â–ˆ| 199859/199859 [00:10<00:00, 18558.30\n",
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|â–ˆ| 199859/199859 [00:18<00:0\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/nico/pytorch/env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-01 18:59:02\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "  0%|                                                  | 0/8496 [00:00<?, ?it/s]/Users/nico/pytorch/env/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  0%|                                    | 12/8496 [26:20<306:34:04, 130.09s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!autotrain setup\n",
    "\n",
    "# setup training parameters\n",
    "project_name = 'my_autotrained_sentiment_predictor_llm'\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "push_to_hub = False\n",
    "# if you cannot find your token, check\n",
    "# https://huggingface.co/settings/tokens\n",
    "# then set your token with:\n",
    "# conda env config vars set HUGGINGFACE_TOKEN=token_goes_here\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\") or \"type_your_token\"\n",
    "repo_id = \"schroedinger-s-cat/product_rating_predictor\"\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "block_size = 1024\n",
    "trainer = \"sft\"\n",
    "warmup_ratio = 0.1\n",
    "weight_decay = 0.01\n",
    "gradient_accumulation = 4\n",
    "use_fp16 = False\n",
    "use_peft = True\n",
    "use_int4 = False\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.045\n",
    "\n",
    "# propagate parameters to the local environment\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
    "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
    "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"USE_PEFT\"] = str(use_peft)\n",
    "os.environ[\"USE_INT4\"] = str(use_int4)\n",
    "os.environ[\"LORA_R\"] = str(lora_r)\n",
    "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
    "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n",
    "\n",
    "# if needed, check auto train cmd line params:\n",
    "# https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/cli/run_llm.py\n",
    "# run AutoTrain\n",
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--epochs ${NUM_EPOCHS} \\\n",
    "--block-size ${BLOCK_SIZE} \\\n",
    "--warmup-ratio ${WARMUP_RATIO} \\\n",
    "--lora-r ${LORA_R} \\\n",
    "--lora-alpha ${LORA_ALPHA} \\\n",
    "--lora-dropout ${LORA_DROPOUT} \\\n",
    "--weight-decay ${WEIGHT_DECAY} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--mixed-precision fp16\" ) \\\n",
    "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
    "$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--quantization int4\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model directly\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = project_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "review_text = \"This book is great value.\\n\\\n",
    "A must buy if you are into reading\"\n",
    "input_text = f\"\\\n",
    "[INST] Below is an instruction that describes a task, \\\n",
    "paired with an input that provides context for the task. \\\n",
    "Write a response that appropriately completes the request. \\\n",
    "\\n\\n### Instruction:\\n{INSTRUCTION} \\\n",
    "\\n\\n### Input:\\n <product_review>{review_text}</product_review> [/INST]\"\n",
    "input = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generateCompletion(input, max_new_tokens=5)\n",
    "result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model via pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer)\n",
    "result = pipe(input_text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch211",
   "language": "python",
   "name": "pytorch211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
