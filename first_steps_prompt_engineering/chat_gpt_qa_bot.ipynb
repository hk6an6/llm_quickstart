{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Q/A bot with LLMs\n",
    "\n",
    "We'll build a Q/A bot with an open source alternative to ChatGPT. There are many options to choose from (see alternatives), and we'll use Mixtral7b:intsruct, because it can run on a Mac, it's open source, and we don't need to send data outside our computer. This simplifies the implementation and means we don't need to pay for API tokens from openAI. Mixtral7b's performance beats LLAMA2, which is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to chatGPT\n",
    "These models can be configured for serving with [ollama](https://ollama.ai/library) and [llamacpp](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file):\n",
    "\n",
    "1. Instruction tuned T5: https://huggingface.co/google/flan-t5-large\n",
    "1. Few shot learner: https://huggingface.co/EleutherAI/gpt-j-6b\n",
    "1. Instruction tuned model that outperforms LLAMA2: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "1. One shot learner: https://huggingface.co/adept/persimmon-8b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Mixtral7b with Hugging face\n",
    "\n",
    "This is an example for using Mixtral via Huggingface:\n",
    "\n",
    "```\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"mps\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n",
    "\n",
    "```\n",
    "\n",
    "This is great, but we'll decouple hosting the model from using the model.\n",
    "\n",
    "## Getting a hosted version of mixtral\n",
    "\n",
    "1. Download and install OLLAMA from: https://ollama.ai/\n",
    "1. Install mixtral `curl http://localhost:11434/api/pull -d '{ \"name\": \"mistral:instruct\" }'`\n",
    "1. Run `conda install requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The color of the sky appears blue due to a phenomenon called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with different gases and particles present in the air. Blue light has a shorter wavelength and gets scattered more easily than other colors due to its smaller size. Consequently, when we look up at the sky, we primarily see the blue light that has been scattered in all directions.\n"
     ]
    }
   ],
   "source": [
    "# '''\n",
    "# curl http://localhost:11434/api/generate -d '{\n",
    "#   \"model\": \"mistral\",\n",
    "#   \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n",
    "#   \"raw\": true,\n",
    "#   \"stream\": false\n",
    "# }'\n",
    "# '''\n",
    "\n",
    "\n",
    "# format: the format to return a response in. Currently the only accepted value is json\n",
    "# options: additional model parameters listed in the documentation for the Modelfile such as temperature\n",
    "# system: system message to (overrides what is defined in the Modelfile)\n",
    "# template: the full prompt or prompt template (overrides what is defined in the Modelfile)\n",
    "# context: the context parameter returned from a previous request to /generate, this can be used to keep a short conversational memory\n",
    "# stream: if false the response will be returned as a single response object, rather than a stream of objects\n",
    "# raw: if true no formatting will be applied to the prompt. You may choose to use the raw parameter if you are specifying a full templated prompt in your request to the API.\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "import json\n",
    "import requests\n",
    "payload = { \\\n",
    "  'model': 'mistral', \\\n",
    "  'prompt': \"[INST] why is the sky blue? [/INST]\", \\\n",
    "  'raw': True, \\\n",
    "  'stream': False \\\n",
    "}\n",
    "r = requests.post(OLLAMA_URL, data=json.dumps(payload))\n",
    "print(json.loads(r.text)['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "You are a helpful Q/A bot that can only reference material from a knowledge base.\n",
    "All context will be retrieved from a knowledge base.\n",
    "For any questions not \"from the knowledge base\", say that you cannot answer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch211",
   "language": "python",
   "name": "pytorch211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
