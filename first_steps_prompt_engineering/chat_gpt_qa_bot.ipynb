{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Q/A bot with LLMs\n",
    "\n",
    "We'll build a Q/A bot with an open source alternative to ChatGPT. There are many options to choose from (see alternatives), and we'll use Mixtral7b:intsruct, because it can run on a Mac, it's open source, and we don't need to send data outside our computer. This simplifies the implementation and means we don't need to pay for API tokens from openAI. Mixtral7b's performance beats LLAMA2, which is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to chatGPT\n",
    "These models can be configured for serving with [ollama](https://ollama.ai/library) and [llamacpp](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file):\n",
    "\n",
    "1. Instruction tuned T5: https://huggingface.co/google/flan-t5-large\n",
    "1. Few shot learner: https://huggingface.co/EleutherAI/gpt-j-6b\n",
    "1. Instruction tuned model that outperforms LLAMA2: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "1. One shot learner: https://huggingface.co/adept/persimmon-8b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Mixtral7b with Hugging face\n",
    "\n",
    "This is an example for using Mixtral via Huggingface. It's ok to skip the example but I'm keeping it fore reference.\n",
    "\n",
    "The reason it's ok to skip the example is that we'll use a hosted version of the model instead of directly calling the model with Huggingface APIs. The calls to the model will make use of OLLAMA's RESTful API. For more info on using OLLAMA's RESTful API, see the following section.\n",
    "\n",
    "```\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"mps\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n",
    "\n",
    "```\n",
    "\n",
    "This is great, but we'll decouple hosting the model from using the model.\n",
    "\n",
    "## Using OLLAMA's API\n",
    "\n",
    "\n",
    "OLLAMA's restful API works by making HTTP posts with the information requested. For example:\n",
    "\n",
    "'''\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"mistral\",\n",
    "  \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n",
    "  \"raw\": true,\n",
    "  \"stream\": false\n",
    "}'\n",
    "'''\n",
    "\n",
    "See the next section for how to make sure you are running OLLAMA and that MISTRAL is hosted on your local environment.\n",
    "\n",
    "## Getting a hosted version of mixtral\n",
    "\n",
    "1. Download and install OLLAMA from: https://ollama.ai/\n",
    "1. Install mixtral `curl http://localhost:11434/api/pull -d '{ \"name\": \"mistral:instruct\" }'`\n",
    "1. Run `conda install requests`.\n",
    "\n",
    "Once you are ready to call Mistral on your local environment, you are ready to try the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The color of the sky appears blue due to a process called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with molecules and particles in the air, such as nitrogen and oxygen. These particles scatter the shorter wavelengths of light, primarily blue, more than other colors because they are smaller in size than the wavelengths of light. Consequently, the sky appears blue during the day. However, at sunrise or sunset, when the sunlight has to pass through more of the atmosphere, the scattering of longer wavelengths, such as red and orange, becomes more pronounced, resulting in the beautiful colors we observe in the sky during those times.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def generateCompletion(prompt:str,\n",
    "                       model:str='mistral:instruct',\n",
    "                       server_url:str=\"http://localhost:11434/api/generate\")->str:\n",
    "  payload = {\n",
    "    'model': model,\n",
    "    'prompt': \"[INST]%s[/INST]\" % prompt,\n",
    "    'raw': True,\n",
    "    'stream': False\n",
    "  }\n",
    "  r = requests.post(server_url, data=json.dumps(payload))\n",
    "  return json.loads(r.text)['response']\n",
    "\n",
    "print(generateCompletion('why is the sky blue?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulting the knowledge base\n",
    "\n",
    "The code beneath has functions that consult the knowledge base and prepare knowledge base contents for calling the language model. The knowledge base contents will be provided as context when asking questions to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"article_0\": \" which of all the virtues is the proper\\nvirtue for this present use\", \"article_1\": \" suffice\\nit to say, Herodes succeeded in defending himself to the satisfaction of\\nthe emperor\", \"article_2\": \" as whether meekness, fortitude, truth,\\nfaith, sincerity, contentation, or any of the rest\", \"article_3\": \" His care to preserve\\nhis friends\", \"article_4\": \"Fronto replied, thanking the prince for his advice, and promising that\\nhe will confine himself to the facts of the case. But he points out that\\nthe charges brought against Herodes were such, that they can hardly be\\nmade agreeable; amongst them being spoliation, violence, and murder\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy import array as nparray\n",
    "tokenizer = SentenceTransformer(\n",
    "  'sentence-transformers/multi-qa-mpnet-base-cos-v1'\n",
    ")\n",
    "\n",
    "def makeEmbedding(chunks:list[str],\n",
    "    tokenizer:SentenceTransformer=tokenizer)->list[nparray]:\n",
    "  '''\n",
    "  returns a list of vectors. Vectors are numpy arrays.\n",
    "  chunks is a list of strings with text\n",
    "  model is 'sentence-transformers/multi-qa-mpnet-base-cos-v1' and warpped in SentenceTransformer\n",
    "  '''\n",
    "  return tokenizer.encode(\n",
    "    [chunk for chunk in chunks]\n",
    "    , batch_size=32\n",
    "    , device='mps' # send work to Metal shaders in M1 macs\n",
    "    , show_progress_bar=True\n",
    "  )\n",
    "\n",
    "def findMatches(query:str,\n",
    "                host:str=\"127.0.0.1\",\n",
    "                database:str=\"semantic_search\",\n",
    "                user:str=\"postgres\",\n",
    "                password:str=\"123456\")->list[str]:\n",
    "  '''\n",
    "  returns matches from the knowledge base.\n",
    "  query is a question to be answered by finding mathces in the knowledge base.\n",
    "  '''\n",
    "  q_encoded = makeEmbedding([query])[0].tolist()\n",
    "  CMD = \"select text_chunk from items \\\n",
    "    order by embedding <=> '%s' limit 5\"\n",
    "  results = None\n",
    "  with psycopg2.connect(\n",
    "    host=host,\n",
    "    database=database,\n",
    "    user=user,\n",
    "    password=password\n",
    "  ) as connection:\n",
    "    with connection.cursor() as cursor:\n",
    "      cursor.execute(CMD % q_encoded)\n",
    "      results = cursor.fetchall()\n",
    "  return [result[0] for result in results]\n",
    "\n",
    "def asArticles(statements:list[str])->str:\n",
    "  articles = {\"article_%s\"%i:statements[i] for i in range(len(statements))}\n",
    "  return json.dumps(articles)\n",
    "\n",
    "results = asArticles(findMatches(\"what are the virtues of a hero?\"))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the language model with the knowledge base\n",
    "\n",
    "The code below queries the knowledge base and uses prompt engineering to ask the language model to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the knowledge base, virtue is a quality or disposition that is considered good and desirable. It consists not in passion but in action (XIV). The true good or evil of a reasonable and charitable man does not consist in passion but in operation and action (XIV). However, the specific virtue for the present use is mentioned as something different and more excellent and divine than the motions of the elements (XVI), but it is not explicitly defined as such in the provided passages. The knowledge base also mentions that the virtue of suffering in itself is not discussed in the _Meditations_.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "You are a helpful Q/A bot that can only reference material from a knowledge base.\n",
    "All context will be retrieved from articles in a knowledge base. This is the knowledge base:\n",
    "<knowledge_base>%s</knowledge_base>\n",
    "For any questions not \"from the knowledge base\", say that you cannot answer.\n",
    "<question>%s</question>\n",
    "'''\n",
    "\n",
    "def answer(question:str)->str:\n",
    "  knowledge_base = asArticles(findMatches(question))\n",
    "  prompt = SYSTEM_PROMPT % (findMatches(question),question)\n",
    "  return generateCompletion(prompt)\n",
    "\n",
    "print(answer('What is virtue?').strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch211",
   "language": "python",
   "name": "pytorch211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
